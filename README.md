# novel-video-synthesis
[Base mode](https://github.com/DeepBinder-main/hotshot-xl)



## References & citations

Spatial & Temporal Transformer 
- https://research.nvidia.com/labs/toronto-ai/VideoLDM/

https://huggingface.co/hotshotco/Hotshot-XL
https://github.com/PKU-YuanGroup/Open-Sora-Plan

ControlNet
- https://github.com/lllyasviel/ControlNet
  
Distributive training
- https://huggingface.co/docs/accelerate/

https://github.com/AUTOMATIC1111/stable-diffusion-webui
https://github.com/comfyanonymous/ComfyUI
https://github.com/facebookresearch/xformers
https://github.com/Dao-AILab/flash-attention

[FiT: Flexible Vision Transformer for Diffusion Model](https://github.com/whlzy/FiT)

SOTA Caption Generation for video : https://github.com/snap-research/Panda-70M
https://github.com/willisma/SiT

Unet 
- https://huggingface.co/docs/diffusers/en/api/loaders/unet
- https://paperswithcode.com/method/u-net

https://github.com/Vchitect/Latte


![image](https://github.com/DeepBinder-main/DiffScaler/assets/97831658/7ae2b077-b09b-4be4-b39b-8616f9328a61)

AnimateDiff + freeinit
- https://github.com/guoyww/animatediff/
- https://dagshub.com/ByteDance/AnimateDiff-Lightning
- https://github.com/TianxingWu/FreeInit
